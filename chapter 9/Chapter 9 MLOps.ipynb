{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps with CLI V2\n",
    "\n",
    "- Creating Scripts for MLOps Pipeline\n",
    "- The kernel you use for this notebook can be your job_env or the Python 3.10 SDK V2\n",
    "\n",
    "## In this notebook we will:\n",
    "\n",
    "- Create base folders for holding the scripts in your pipeline steps\n",
    "- Create a pipeline to create and register a model\n",
    "- Register the raw data (which your pipeline will transform)\n",
    "- Create scripts for pipeline steps to prep your data, train your model, evaluate your model\n",
    "- Create scripts for deploying your model with your devOps pipeline\n",
    "\n",
    "You can use the **Python 3.10 - SDK V2** environment to run this notebook, as it is just creating the files that are needed for your Azure DevOps pipeline\n",
    "\n",
    "To work with this notebook you will need to have the following:\n",
    "- An Azure DevOps Service Connection set to: `aml-dev`\n",
    "- An Azure DevOps Service Connection set to: `aml-qa`\n",
    "- An Azure DevOps Environment set to qa\n",
    "- An Azure DevOps Variable Group Called: `devops-variable-group-dev`\n",
    "    - Variables in the Azure DevOps Variable Group include:\n",
    "        - `location` (ex: eastus)\n",
    "        - `resourceGroup` (the name of your AMLS workspace resource group connected with your aml-dev service connection)\n",
    "        - `wsName` (the name of your AMLS workspace resource group connected with your aml-dev service connection)\n",
    "- An Azure DevOps Variable Group Called: `devops-variable-group-qa`\n",
    "    - Variables in the Azure DevOps Variable Group include:\n",
    "        - `location` (ex: eastus)\n",
    "        - `resourceGroup` (the name of your AMLS workspace resource group connected with your aml-dev service connection)\n",
    "        - `wsName` (the name of your AMLS workspace resource group connected with your aml-dev service connection)\n",
    "        \n",
    "Locations are defined here: https://github.com/microsoft/azure-pipelines-extensions/blob/master/docs/authoring/endpoints/workspace-locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml.entities import Environment, BuildContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '<SUBSCRIPTION_ID>'\n",
    "resource_group= '<resource_group>'\n",
    "workspace = '<workspace>'\n",
    "\n",
    "subscription_id =  \"5da07161-3770-4a4b-aa43-418cbbb627cf\"\n",
    "resource_group =  \"mm-ch9aml-dev-rg\"\n",
    "workspace =  \"mm-ch9aml-dev\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n",
      "data asset is registered\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('./data/titanic.csv')\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "\n",
    "try:\n",
    "    registered_data_asset = ml_client.data.get(name='titanic_raw', version=1)\n",
    "    print('data asset is registered')\n",
    "except:\n",
    "    print('register data asset')\n",
    "    my_data = Data(\n",
    "        path=\"./data/titanic.csv\",\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        description=\"Titanic CSV\",\n",
    "        name=\"titanic_raw\",\n",
    "        version=\"1\",\n",
    "    )\n",
    "\n",
    "    ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./src/ src folder created\n",
      "./src/prep prep folder created\n",
      "./src/train train folder created\n",
      "./src/eval eval folder created\n",
      "./src/deploy deploy folder created\n",
      "./src/pipeline pipeline folder created\n",
      "./src/conda-yamls conda-yamls folder created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "script_folder = './src/'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'src folder created')\n",
    "\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "script_folder = './src/prep'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'prep folder created')\n",
    "\n",
    "script_folder = './src/train'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'train folder created')\n",
    "\n",
    "script_folder = './src/eval'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'eval folder created')\n",
    "\n",
    "script_folder = './src/deploy'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'deploy folder created')\n",
    "\n",
    "script_folder = './src/pipeline'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'pipeline folder created')\n",
    "\n",
    "script_folder = './src/conda-yamls'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'conda-yamls folder created')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Compute Cluster for Training in Dev Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# specify aml compute name.\n",
    "cpu_compute_target = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    ml_client.compute.get(cpu_compute_target)\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "    compute = AmlCompute(\n",
    "        name=cpu_compute_target, size=\"STANDARD_D2_V2\", min_instances=0, max_instances=4, idle_time_before_scale_down = 3600\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Scripts for Training Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/conda-yamls/pipeline_conda_env.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/conda-yamls/pipeline_conda_env.yml\n",
    "name: job_env\n",
    "dependencies:\n",
    "- python=3.10\n",
    "- scikit-learn=1.1.3\n",
    "- ipykernel\n",
    "- matplotlib\n",
    "- pandas\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults==1.48.0 #needed for the inferece schema\n",
    "  - mlflow<=1.30.0\n",
    "  - azure-ai-ml==1.1.2\n",
    "  - mltable==1.0.0\n",
    "  - azureml-mlflow==1.48.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/prep/prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/prep/prep.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "parser = argparse.ArgumentParser(\"prep\")\n",
    "parser.add_argument(\"--raw_data\", type=str, help=\"Path to raw data\")\n",
    "parser.add_argument(\"--prep_data\", type=str, help=\"Path of prepped data\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args.raw_data)\n",
    "print(args.prep_data)\n",
    "\n",
    "df = pd.read_csv(args.raw_data + '/titanic.csv')\n",
    "\n",
    "df['Age'] = df.groupby(['Pclass', 'Sex'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
    "df['Sex']= df['Sex'].apply(lambda x: x[0] if pd.notnull(x) else 'X')\n",
    "df['Loc']= df['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'X')\n",
    "df.drop(['Cabin', 'Ticket'], axis=1, inplace=True)\n",
    "df['Embarked'] = df['Embarked'].fillna('S')\n",
    "df.loc[:,'GroupSize'] = 1 + df['SibSp'] + df['Parch']\n",
    "\n",
    "df_train = df\n",
    "df = df_train.drop(['Name','SibSp', 'Parch', 'PassengerId'], axis=1)\n",
    "\n",
    "df.to_csv(args.prep_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/train/train.py\n",
    "import os\n",
    "import argparse\n",
    "import joblib\n",
    "import shutil\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# define functions\n",
    "def main(args):\n",
    "    current_run = mlflow.start_run()\n",
    "    mlflow.sklearn.autolog(log_models=False)\n",
    "\n",
    "    # read in data\n",
    "    print('about to read file:' + args.prep_data)\n",
    "    df = pd.read_csv(args.prep_data)\n",
    "    model, X_test, y_test = model_train('Survived', df, 0)\n",
    "    \n",
    "    model_file = os.path.join(args.model_output, 'titanic_model.pkl')\n",
    "    joblib.dump(value=model, filename=model_file)\n",
    "    \n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    y_test.to_csv('outputs/Y_test.csv', index = False)\n",
    "    X_test.to_csv( 'outputs/X_test.csv', index = False)\n",
    "    shutil.copytree('./outputs/', args.test_data, dirs_exist_ok=True)\n",
    "\n",
    "def model_train(LABEL, df, randomstate):\n",
    "    print('df.columns = ')\n",
    "    print(df.columns)\n",
    "    \n",
    "    df['Embarked'] = df['Embarked'].astype(object)\n",
    "    df['Loc'] = df['Loc'].astype(object)\n",
    "    df['Loc'] = df['Sex'].astype(object)\n",
    "    df['Pclass'] = df['Pclass'].astype(float)\n",
    "    df['Age'] = df['Age'].astype(float)\n",
    "    df['Fare'] = df['Fare'].astype(float)\n",
    "    df['GroupSize'] = df['GroupSize'].astype(float)\n",
    "    \n",
    "    y_raw           = df[LABEL]\n",
    "    columns_to_keep = ['Embarked', 'Loc', 'Sex','Pclass', 'Age', 'Fare', 'GroupSize']\n",
    "    X_raw           = df[columns_to_keep]\n",
    "    \n",
    "\n",
    "    print(X_raw.columns)\n",
    "     # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.2, random_state=args.randomstate)\n",
    "    \n",
    "    #use Logistic Regression estimator from scikit learn\n",
    "    lg = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')\n",
    "    preprocessor = buildpreprocessorpipeline(X_train)\n",
    "    \n",
    "    #estimator instance\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', lg)], verbose=True)\n",
    "\n",
    "    model = clf.fit(X_train, y_train)\n",
    "    \n",
    "    print('type of X_test = ' + str(type(X_test)))\n",
    "          \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print('*****X_test************')\n",
    "    print(X_test)\n",
    "    \n",
    "    #get the active run.\n",
    "    run = mlflow.active_run()\n",
    "    print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "    acc = model.score(X_test, y_test )\n",
    "    print('Accuracy:', acc)\n",
    "    MlflowClient().log_metric(run.info.run_id, \"test_acc\", acc)\n",
    "    \n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "    print('AUC: ' , auc)\n",
    "    MlflowClient().log_metric(run.info.run_id, \"test_auc\", auc)\n",
    "    \n",
    "    \n",
    "    # Signature\n",
    "    signature = infer_signature(X_test, y_test)\n",
    "\n",
    "    # Conda environment\n",
    "    custom_env =_mlflow_conda_env(\n",
    "        additional_conda_deps=[\"scikit-learn==1.1.3\"],\n",
    "        additional_pip_deps=[\"mlflow<=1.30.0\"],\n",
    "        additional_conda_channels=None,\n",
    "    )\n",
    "\n",
    "    # Sample\n",
    "    input_example = X_train.sample(n=1)\n",
    "\n",
    "    # Log the model manually\n",
    "    mlflow.sklearn.log_model(model, \n",
    "                             artifact_path=\"championmodel\", \n",
    "                             conda_env=custom_env,\n",
    "                             signature=signature,\n",
    "                             input_example=input_example)\n",
    "\n",
    "    return model, X_test, y_test\n",
    "\n",
    "\n",
    "def buildpreprocessorpipeline(X_raw):\n",
    "\n",
    "    categorical_features = X_raw.select_dtypes(include=['object', 'bool']).columns\n",
    "    numeric_features = X_raw.select_dtypes(include=['float','int64']).columns\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[('onehotencoder', \n",
    "                                               OneHotEncoder(categories='auto', sparse=False, handle_unknown='ignore'))])\n",
    "\n",
    "\n",
    "    numeric_transformer1 = Pipeline(steps=[('scaler1', SimpleImputer(missing_values=np.nan, strategy = 'mean'))])\n",
    "    \n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('numeric1', numeric_transformer1, numeric_features),\n",
    "            ('categorical', categorical_transformer, categorical_features)], remainder='drop')\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--prep_data\", default=\"data\", type=str, help=\"Path to prepped data, default to local folder\")\n",
    "    parser.add_argument(\"--input_file_name\", type=str, default=\"titanic.csv\")\n",
    "    parser.add_argument(\"---randomstate\", type=int, default=42)\n",
    "    \n",
    "    parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
    "    parser.add_argument(\"--test_data\", type=str,)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "    print(args.prep_data)\n",
    "    print(args.input_file_name)\n",
    "    print(args.randomstate)\n",
    "    print(args.model_output)\n",
    "    print(args.test_data)\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/eval/evaluatemodel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/eval/evaluatemodel.py\n",
    "import argparse\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import joblib\n",
    "import shutil\n",
    "from azureml.core import Run\n",
    "from azureml.core import Model\n",
    "\n",
    "    \n",
    "    \n",
    "# define functions\n",
    "def main(args):\n",
    "    \n",
    "    model_name = args.model_name\n",
    "    \n",
    "    run = Run.get_context()\n",
    "    ws = run.experiment.workspace\n",
    "    run_id = run.id\n",
    "    print('run_id =' + run_id)\n",
    "    model_list = Model.list(ws, name=model_name, latest=True)\n",
    "    first_registration = len(model_list)==0\n",
    "    current_model = None\n",
    "        \n",
    "    # read in data\n",
    "    print('about to read file:' + args.test_data)\n",
    "    X_test = pd.read_csv(args.test_data + '/X_test.csv')\n",
    "    df_y_test = pd.read_csv(args.test_data + '/Y_test.csv')\n",
    "    y_test  = df_y_test.values.flatten()\n",
    "    #load champion model\n",
    "    model_file = os.path.join(args.model_folder, 'titanic_model.pkl')\n",
    "    champion_model = joblib.load(model_file)\n",
    "    \n",
    "    y_pred_current = champion_model.predict(X_test)\n",
    "    print('y_pred_current')\n",
    "    print(y_pred_current)\n",
    "    print('y_test')\n",
    "    print(y_test)\n",
    "    \n",
    "    champion_auc = roc_auc_score(y_test,y_pred_current)\n",
    "    print('champion_auc:' , champion_auc)\n",
    "    \n",
    "    champion_acc = np.average(y_pred_current == y_test)\n",
    "    print('champion_acc:', champion_acc)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        current_model_aml = Model(ws,args.model_name)\n",
    "        os.makedirs(\"current_model\", exist_ok=True)\n",
    "        current_model_aml.download(\"current_model\",exist_ok=True)\n",
    "        current_model = mlflow.sklearn.load_model(os.path.join(\"current_model\",args.model_name))\n",
    "    except:\n",
    "        print('no model register with name' + args.model_name)\n",
    "        pass\n",
    "    \n",
    "    if current_model:\n",
    "        y_pred_current = current_model.predict(X_test)\n",
    "        current_acc = np.average(y_pred_current == y_test)\n",
    "        print('current_acc:', current_acc)\n",
    "        if champion_acc >= current_acc:\n",
    "            print('better model found, registering')\n",
    "\n",
    "            # Signature\n",
    "            signature = infer_signature(X_test, y_test)\n",
    "\n",
    "            # Conda environment\n",
    "            custom_env =_mlflow_conda_env(\n",
    "                additional_conda_deps=[\"scikit-learn==1.1.3\"],\n",
    "                additional_pip_deps=[\"mlflow<=1.30.0\"],\n",
    "                additional_conda_channels=None,\n",
    "            )\n",
    "\n",
    "            # Sample\n",
    "            input_example = X_test.sample(n=1)\n",
    "\n",
    "            # Log the model manually\n",
    "            mlflow.sklearn.log_model(champion_model, \n",
    "                                     artifact_path=args.model_name, \n",
    "                                     conda_env=custom_env,\n",
    "                                     signature=signature,\n",
    "                                     input_example=input_example)\n",
    "            model_uri = f'runs:/{run_id}/{args.model_name}'\n",
    "            mlflow.register_model(model_uri,args.model_name)\n",
    "            ##########################################################\n",
    "    \n",
    "            \n",
    "        else:\n",
    "            print('current model performs better than champion model ')\n",
    "            print('champion_acc:', champion_acc)\n",
    "            print('current_acc:', current_acc)\n",
    "    else:\n",
    "        print('no current model')\n",
    "        print(\"First time model train, registering\")\n",
    "\n",
    "        signature = infer_signature(X_test, y_test)\n",
    "\n",
    "        # Conda environment\n",
    "        custom_env =_mlflow_conda_env(\n",
    "                additional_conda_deps=[\"scikit-learn==1.1.3\"],\n",
    "                additional_pip_deps=[\"mlflow<=1.30.0\"],\n",
    "                additional_conda_channels=None,\n",
    "            )\n",
    "\n",
    "        # Sample\n",
    "        input_example = X_test.sample(n=1)\n",
    "\n",
    "        # Log the model manually\n",
    "        mlflow.sklearn.log_model(champion_model, \n",
    "                                 artifact_path=args.model_name, \n",
    "                                 conda_env=custom_env,\n",
    "                                 signature=signature,\n",
    "                                 input_example=input_example)\n",
    "        model_uri = f'runs:/{run_id}/{args.model_name}'\n",
    "        mlflow.register_model(model_uri,args.model_name)\n",
    "        \n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--test_data\", default=\"data\", type=str, help=\"Path to test data\")\n",
    "    parser.add_argument(\"--model_folder\", default=\"data\", type=str, help=\"Path to model data\")\n",
    "    parser.add_argument(\"--model_name\",default='mmchapter9titanic',type=str, help=\"Name of the model in workspace\")\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(args.test_data)\n",
    "    print(args.model_folder)\n",
    "    print(args.model_name)\n",
    "        \n",
    "    return args\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AML Pipeline Definition\n",
    "\n",
    "- This pipeline defines the AML Pipeline job to:\n",
    "    - Prep\n",
    "    - Train\n",
    "    - Evaluate and Register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/pipeline/aml_train_and_eval_pipeline.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/pipeline/aml_train_and_eval_pipeline.yml\n",
    "\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\n",
    "type: pipeline\n",
    "display_name: Training_and_eval_pipeline\n",
    "compute: azureml:cpu-cluster\n",
    "\n",
    "\n",
    "jobs:\n",
    "  prep_job:\n",
    "    type: command\n",
    "    code: ../prep\n",
    "    command: >-\n",
    "      python prep.py \n",
    "      --raw_data ${{inputs.raw_data}}\n",
    "      --prep_data ${{outputs.prep_data}}\n",
    "    inputs:\n",
    "      raw_data:\n",
    "        type: uri_folder\n",
    "        path: azureml:titanic_raw:1\n",
    "        mode: ro_mount\n",
    "    outputs:\n",
    "      prep_data:\n",
    "        type: uri_file\n",
    "        path: azureml://datastores/workspaceblobstore/paths/titanic_prep_data/titanic_prepped.csv\n",
    "        mode: rw_mount\n",
    "    environment:\n",
    "      conda_file: ../conda-yamls/pipeline_conda_env.yml\n",
    "      image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\n",
    "\n",
    "    \n",
    "  train_job:\n",
    "    type: command\n",
    "    inputs:\n",
    "      prep_data: ${{parent.jobs.prep_job.outputs.prep_data}}\n",
    "    outputs:\n",
    "      model_output:\n",
    "        type: uri_folder\n",
    "        path: azureml://datastores/workspaceblobstore/paths/titanic_model_data/\n",
    "        mode: rw_mount\n",
    "      test_data: \n",
    "        type: uri_folder\n",
    "        path: azureml://datastores/workspaceblobstore/paths/titanic_test_data/\n",
    "        mode: rw_mount\n",
    "    code: ../train\n",
    "    environment:\n",
    "      conda_file: ../conda-yamls/pipeline_conda_env.yml\n",
    "      image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\n",
    "    compute: azureml:cpu-cluster\n",
    "    command: >-\n",
    "      python train.py \n",
    "      --prep_data ${{inputs.prep_data}} \n",
    "      --model_output ${{outputs.model_output}}\n",
    "      --test_data ${{outputs.test_data}}\n",
    "\n",
    "  eval_job:\n",
    "    type: command\n",
    "    inputs:\n",
    "      test_data: ${{parent.jobs.train_job.outputs.test_data}}\n",
    "      model_folder: ${{parent.jobs.train_job.outputs.model_output}}\n",
    "    code: ../eval\n",
    "    environment:\n",
    "      conda_file: ../conda-yamls/pipeline_conda_env.yml\n",
    "      image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\n",
    "    compute: azureml:cpu-cluster\n",
    "    command: >-\n",
    "      python evaluatemodel.py \n",
    "      --test_data ${{inputs.test_data}} \n",
    "      --model_folder ${{inputs.model_folder}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Deployment files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be sure to change your endpoint name to be something that wil be unique\n",
    "\n",
    "- change the vlaue of **name** to be something unique to ensure the deployment will succeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/deploy/create-endpoint.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/deploy/create-endpoint.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json \n",
    "name: xmmchapter9titanicendpoint\n",
    "auth_mode: key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/deploy/create-endpoint-dev.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/deploy/create-endpoint-dev.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json \n",
    "name: xmmchapter9titanicendpointdev\n",
    "auth_mode: key "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be sure to change your endpoint name to match the endpoint name in the `create-endpoint.yml` file\n",
    "\n",
    "- change the vlaue of **endpoint_name** to be the same as in the create-endpoint.yml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/deploy/model_deployment.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/deploy/model_deployment.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json \n",
    "name: green\n",
    "endpoint_name: xmmchapter9titanicendpoint\n",
    "model: azureml:mmchapter9titanic@latest \n",
    "instance_type: Standard_DS2_v2 \n",
    "instance_count: 1  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/deploy/model_deployment-dev.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/deploy/model_deployment-dev.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json \n",
    "name: green\n",
    "endpoint_name: xmmchapter9titanicendpointdev\n",
    "model: azureml:mmchapter9titanic@latest \n",
    "instance_type: Standard_DS2_v2 \n",
    "instance_count: 1  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure DevOps Pipeline\n",
    "### Be sure to change your endpoint name to match the endpoint name in the `create-endpoint.yml` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/AzureDevOpsPipeline.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/AzureDevOpsPipeline.yml\n",
    "\n",
    "resources:\n",
    "  containers:\n",
    "  - container: mlops\n",
    "    image: mcr.microsoft.com/mlops/python:latest\n",
    "\n",
    "pr: none\n",
    "trigger:\n",
    "  branches:\n",
    "    include:\n",
    "    - main\n",
    "\n",
    "variables:\n",
    "- group: devops-variable-group-dev\n",
    "- group: devops-variable-group-qa\n",
    "- name: model_name\n",
    "  value: mmchapter9titanic\n",
    "\n",
    "- name: ENDPT_NAME\n",
    "  value: xmmchapter9titanicendpoint\n",
    "    \n",
    "- name: DEV_ENDPT_NAME\n",
    "  value: xmmchapter9titanicendpointdev\n",
    "\n",
    "\n",
    "pool:\n",
    "  vmImage: ubuntu-latest\n",
    "\n",
    "stages:\n",
    "- stage: 'DevRunPipline'\n",
    "  variables:\n",
    "  - group: devops-variable-group-dev\n",
    "  displayName: 'DevTrainingPipeline'\n",
    "  jobs:\n",
    "  - job: \"TrainingPipeline\"\n",
    "    steps:  \n",
    "      - task: AzureCLI@1\n",
    "        env:\n",
    "          wsName: $(wsName)\n",
    "          resourceGroup: $(resourceGroup)\n",
    "          location: $(location)\n",
    "        inputs:\n",
    "          azureSubscription: aml-dev\n",
    "          scriptLocation: inlineScript\n",
    "          workingDirectory: '$(Build.SourcesDirectory)'\n",
    "          inlineScript: |\n",
    "            echo \"files:\"\n",
    "            ls\n",
    "            az version\n",
    "            az extension add -n ml -y\n",
    "            az version\n",
    "            az configure --defaults group=$(resourceGroup) workspace=$(wsName) location=$(location)\n",
    "            az ml model list -w $(wsName) -g $(resourceGroup)  -n $(model_name) --query \"[0].version\" -o tsv\n",
    "            \n",
    "            if [[ -z \"$(az ml model list -w $(wsName) -g $(resourceGroup)  -n $(model_name) --query '[0].version' -o tsv)\" ]]; then \n",
    "                echo \"no model was found, set value to 0\"\n",
    "                echo \"##vso[task.setvariable variable=modelversion;isOutput=true]0\"\n",
    "                echo \"model does not yet exist in this environment, set the value of the model version to 0\"\n",
    "                exit 0\n",
    "            else\n",
    "                echo \"model was found\"\n",
    "                echo \"##vso[task.setvariable variable=modelversion;isOutput=true]$(az ml model list -w $(wsName) -g $(resourceGroup)  -n $(model_name) --query '[0].version' -o tsv)\"\n",
    "                exit 0\n",
    "            fi\n",
    "        name: 'setversion'\n",
    "        displayName: 'Get Initial Model Version'\n",
    "            \n",
    "      - task: Bash@3\n",
    "        inputs:\n",
    "          workingDirectory: '$(Build.SourcesDirectory)'\n",
    "          targetType: 'inline'\n",
    "          script: |\n",
    "            echo 'modelversion'\n",
    "            echo $(setversion.modelversion)\n",
    "            \n",
    "      - task: AzureCLI@1\n",
    "        timeoutInMinutes: 45\n",
    "        env:\n",
    "          wsName: $(wsName)\n",
    "          resourceGroup: $(resourceGroup)\n",
    "          location: $(location)\n",
    "        inputs:\n",
    "          azureSubscription: aml-dev\n",
    "          scriptLocation: inlineScript\n",
    "          workingDirectory: '$(Build.SourcesDirectory)'\n",
    "          inlineScript: |\n",
    "            echo \"initial model version\"\n",
    "            echo $(setversion.modelversion)\n",
    "            echo \"files\"\n",
    "            ls\n",
    "            az ml job create --file 'chapter 9/src/pipeline/aml_train_and_eval_pipeline.yml' --stream --set settings.force_rerun=True\n",
    "        displayName: 'Training Pipeline'\n",
    "            \n",
    "      - task: AzureCLI@1\n",
    "        env:\n",
    "          wsName: $(wsName)\n",
    "          subscriptionId: $(subscriptionId)\n",
    "          resourceGroup: $(resourceGroup)\n",
    "          location: $(location)\n",
    "        inputs:\n",
    "          azureSubscription: aml-dev\n",
    "          scriptLocation: inlineScript\n",
    "          workingDirectory: '$(Build.SourcesDirectory)'\n",
    "          inlineScript: |\n",
    "            echo \"files:\"\n",
    "            ls\n",
    "            az version\n",
    "            az configure --defaults group=$(resourceGroup) workspace=$(wsName) location=$(location)\n",
    "            az ml model list -w $(wsName) -g $(resourceGroup)  -n $(model_name) --query \"[0].version\" -o tsv\n",
    "            echo \"##vso[task.setvariable variable=finalmodelversion;isOutput=true]$(az ml model list -w $(wsName) -g $(resourceGroup)  -n $(model_name) --query '[0].version' -o tsv)\"\n",
    "            echo \"##vso[task.setvariable variable=devResourceGroup;isOutput=true]$(resourceGroup)\"\n",
    "            echo \"##vso[task.setvariable variable=devWsName;isOutput=true]$(wsName)\"\n",
    "            echo \"##vso[task.setvariable variable=devLocation;isOutput=true]$(location)\"\n",
    "        name: 'setfinalversion'\n",
    "        displayName: 'Get Final Model Version'\n",
    "            \n",
    "      - task: AzureCLI@1\n",
    "        env:\n",
    "          wsName: $(wsName)\n",
    "          subscriptionId: $(subscriptionId)\n",
    "          resourceGroup: $(resourceGroup)\n",
    "          location: $(location)\n",
    "        inputs:\n",
    "          azureSubscription: aml-dev\n",
    "          scriptLocation: inlineScript\n",
    "          workingDirectory: '$(Build.SourcesDirectory)'\n",
    "          inlineScript: |\n",
    "            echo 'initial model version '$(setversion.modelversion)\n",
    "            echo 'final model version   '$(setfinalversion.finalmodelversion)\n",
    "            if [[ $(setversion.modelversion) == $(setfinalversion.finalmodelversion) ]]; then\n",
    "                echo \"##vso[task.setvariable variable=runme;isOutput=true]false\"\n",
    "                exit 0\n",
    "            else\n",
    "                echo \"deploy updated model\"\n",
    "                echo \"##vso[task.setvariable variable=runme;isOutput=true]true\"\n",
    "                az ml model download  -w $(wsName) -g $(resourceGroup)  -n $(model_name) -v $(setfinalversion.finalmodelversion)\n",
    "            fi\n",
    "        name: 'checkversions'\n",
    "        displayName: 'Check Versions'\n",
    "            \n",
    "      - task: AzureCLI@1\n",
    "        env:\n",
    "          wsName: $(wsName)\n",
    "          subscriptionId: $(subscriptionId)\n",
    "          resourceGroup: $(resourceGroup)\n",
    "          location: $(location)\n",
    "        inputs:\n",
    "          azureSubscription: aml-dev\n",
    "          scriptLocation: inlineScript\n",
    "          workingDirectory: '$(Build.SourcesDirectory)'\n",
    "          inlineScript: |\n",
    "            echo \"check versions\"\n",
    "            echo $(checkversions.runme)\n",
    "            \n",
    "      - task: AzureCLI@1\n",
    "        env:\n",
    "          wsName: $(wsName)\n",
    "          resourceGroup: $(resourceGroup)\n",
    "          location: $(location)\n",
    "        inputs:\n",
    "          azureSubscription: aml-dev\n",
    "          scriptLocation: inlineScript\n",
    "          workingDirectory: '$(Build.SourcesDirectory)'\n",
    "          inlineScript: |\n",
    "            set -e\n",
    "            echo 'final model version was:'$(setfinalversion.finalmodelversion)\n",
    "            export NEW_DEPLOYMENT_NAME=deployment`echo $(setfinalversion.finalmodelversion)`v`echo $(date +\"%s\")`\n",
    " \n",
    "            echo $NEW_DEPLOYMENT_NAME\n",
    "            echo $ENDPT_NAME\n",
    "            \n",
    "            az extension add -n ml -y\n",
    "            az version\n",
    "            az configure --defaults group=$(resourceGroup) workspace=$(wsName) location=$(location)\n",
    "\n",
    "            ENDPOINT_EXISTS=$(az ml online-endpoint list -g $(resourceGroup) -w $(wsName) -o tsv --query \"[?name=='$DEV_ENDPT_NAME'][name]\" |  wc -l)\n",
    "            if [[ ENDPOINT_EXISTS -ne 1 ]]; then\n",
    "                echo \"endpoint does not exists\"\n",
    "                az ml online-endpoint create --file 'chapter 9/src/deploy/create-endpoint-dev.yml' -g $(resourceGroup) -w $(wsName) -n $(DEV_ENDPT_NAME)\n",
    "                echo \"creating online deployment\"\n",
    "                az ml online-deployment create --name $NEW_DEPLOYMENT_NAME -f 'chapter 9/src/deploy/model_deployment-dev.yml' -g $(resourceGroup) -w $(wsName) \n",
    "                echo \"updating online endpoint tags\"\n",
    "                az ml online-endpoint update -n $DEV_ENDPT_NAME --set tags.prod=$NEW_DEPLOYMENT_NAME  --traffic \"$NEW_DEPLOYMENT_NAME=100\" -g $(resourceGroup) -w $(wsName)\n",
    "                exit 0\n",
    "            else\n",
    "                echo \"endpoint exists, get deployment that already exists \"\n",
    "                PROD_DEPLOYMENT=$(az ml online-endpoint show -n $DEV_ENDPT_NAME -g $(resourceGroup) -w $(wsName) -o tsv --query \"tags.prod\")\n",
    "                echo $PROD_DEPLOYMENT\n",
    "                az ml online-deployment create -g $(resourceGroup) -w $(wsName) --name $NEW_DEPLOYMENT_NAME -f \"chapter 9/src/deploy/model_deployment-dev.yml\" \n",
    "                #test online-deployment created with sample file.\n",
    "                echo \"test new endpoint\"\n",
    "                az ml online-endpoint invoke -n $DEV_ENDPT_NAME --deployment $NEW_DEPLOYMENT_NAME --request-file \"chapter 9/data/request.json\"\n",
    "            \n",
    "                echo \"---------------------------------------------------------------------------\"\n",
    "                echo \"update tag and traffic\"\n",
    "                az ml online-endpoint update -g $(resourceGroup) -w $(wsName) -n $DEV_ENDPT_NAME --set tags.prod=$NEW_DEPLOYMENT_NAME\n",
    "            \n",
    "                echo \"---------------------------------------------------------------------------\"\n",
    "                echo \"set traffic for origin endpoint to 0 - should be updated, but for completeness\"\n",
    "                az ml online-endpoint update -g $(resourceGroup) -w $(wsName) -n $DEV_ENDPT_NAME --traffic \"$NEW_DEPLOYMENT_NAME=100 $PROD_DEPLOYMENT=0\"  --set tags.prod=$NEW_DEPLOYMENT_NAME\n",
    "\n",
    "                echo \"---------------------------------------------------------------------------\"\n",
    "                echo \"check online endpoints deployed\"\n",
    "                az ml online-endpoint list --query \"[].{Name:name}\"  --output table --resource-group $(resourceGroup) --workspace-name $(wsName)\n",
    "                echo \"delete old online endpoints deployed\"\n",
    "                az ml online-deployment delete -g $(resourceGroup) -w $(wsName) --endpoint $DEV_ENDPT_NAME --name $PROD_DEPLOYMENT --yes --no-wait\n",
    "            fi   \n",
    "        name: 'deploydevmodel'\n",
    "        displayName: 'deploydevmodel'\n",
    "        condition: eq(variables['checkversions.runme'], 'true')\n",
    "            \n",
    "      - task: CopyFiles@2\n",
    "        condition: eq(variables['checkversions.runme'], 'true')\n",
    "        inputs:\n",
    "          sourceFolder: '$(Build.SourcesDirectory)'\n",
    "          contents:  |\n",
    "            '**/chapter 9/src/deploy/**' \n",
    "            '**/chapter 9/src/data/request.json' \n",
    "          targetFolder: '$(Build.ArtifactStagingDirectory)' \n",
    "\n",
    "      - task: PublishBuildArtifacts@1\n",
    "        condition: eq(variables['checkversions.runme'], 'true')\n",
    "        displayName: 'Publish Artifact: drop'\n",
    "        inputs:\n",
    "          ArtifactName: 'drop'\n",
    "          publishLocation: 'Container'\n",
    "          PathtoPublish: '$(Build.ArtifactStagingDirectory)'\n",
    "\n",
    "- stage: 'QAPromoteModel'\n",
    "  dependsOn: DevRunPipline\n",
    "  condition: eq(dependencies.DevRunPipline.outputs['TrainingPipeline.checkversions.runme'], 'true')\n",
    "  variables:\n",
    "  - group: devops-variable-group-qa\n",
    "  displayName: 'QAPromoteModel'\n",
    "  jobs:\n",
    "  - deployment: \"RegisterModelQA\"\n",
    "    environment: qa\n",
    "    variables:\n",
    "      vardevResourceGroup: $[ stageDependencies.DevRunPipline.TrainingPipeline.outputs['setfinalversion.devResourceGroup'] ]\n",
    "      vardevWsName: $[ stageDependencies.DevRunPipline.TrainingPipeline.outputs['setfinalversion.devWsName'] ]\n",
    "      vardevLocation: $[ stageDependencies.DevRunPipline.TrainingPipeline.outputs['setfinalversion.devLocation'] ]\n",
    "      vardevModelVersion: $[ stageDependencies.DevRunPipline.TrainingPipeline.outputs['setfinalversion.finalmodelversion'] ]\n",
    "    strategy:\n",
    "      runOnce:    \n",
    "        deploy:\n",
    "          steps:\n",
    "          - download: current\n",
    "            artifact: drop\n",
    "          - task: PowerShell@2\n",
    "            inputs:\n",
    "              targetType: 'inline'\n",
    "              script: |\n",
    "                ls\n",
    "                ls '$(Pipeline.Workspace)/drop/'\n",
    "\n",
    "          - task: AzureCLI@1\n",
    "            env:\n",
    "              wsName: $(wsName)\n",
    "              resourceGroup: $(resourceGroup)\n",
    "              location: $(location)\n",
    "            inputs:\n",
    "              azureSubscription: aml-dev\n",
    "              scriptLocation: inlineScript\n",
    "              workingDirectory: '$(Build.SourcesDirectory)'\n",
    "              inlineScript: |\n",
    "                  az extension add -n ml -y\n",
    "                  az version\n",
    "                  echo \"model version\"\n",
    "                  echo $(vardevModelVersion)\n",
    "                  az ml model list -w $(vardevWsName) -g $(vardevResourceGroup)  -n $(model_name) --query \"[0].version\" -o tsv\n",
    "                  az ml model download  -w $(vardevWsName) -g $(vardevResourceGroup)  -n $(model_name) -v $(vardevModelVersion)\n",
    "                  ls\n",
    "            name: 'downloadmodel'\n",
    "            displayName: 'downloadmodel'\n",
    "\n",
    "          - task: AzureCLI@1\n",
    "            env:\n",
    "              wsName: $(wsName)\n",
    "              resourceGroup: $(resourceGroup)\n",
    "              location: $(location)\n",
    "            inputs:\n",
    "              azureSubscription: aml-qa\n",
    "              scriptLocation: inlineScript\n",
    "              workingDirectory: '$(Build.SourcesDirectory)'\n",
    "              inlineScript: |\n",
    "                  echo \"files:\"\n",
    "                  ls\n",
    "                  echo \"model version\" $(vardevModelVersion)\n",
    "                  az configure --defaults group=$(resourceGroup) workspace=$(wsName) location=$(location)\n",
    "                  az ml model create --name $(model_name) -v $(vardevModelVersion) --path ./$(model_name)/$(model_name) --type mlflow_model -g $(resourceGroup) -w $(wsName)\n",
    "            name: 'registermodel'\n",
    "            displayName: 'registermodel'\n",
    "          - task: AzureCLI@1\n",
    "            env:\n",
    "              wsName: $(wsName)\n",
    "              resourceGroup: $(resourceGroup)\n",
    "              location: $(location)\n",
    "            inputs:\n",
    "              azureSubscription: aml-qa\n",
    "              scriptLocation: inlineScript\n",
    "              workingDirectory: '$(Build.SourcesDirectory)'\n",
    "              inlineScript: |\n",
    "                  echo 'final model version was:'$(vardevModelVersion)\n",
    "                  export NEW_DEPLOYMENT_NAME=deployment`echo $(vardevModelVersion)`v`echo $(date +\"%s\")`\n",
    "            \n",
    "                  echo $NEW_DEPLOYMENT_NAME\n",
    "                  echo $ENDPT_NAME\n",
    "\n",
    "                  az extension add -n ml -y\n",
    "                  az version\n",
    "\n",
    "                  ENDPOINT_EXISTS=$(az ml online-endpoint list -g $(resourceGroup) -w $(wsName) -o tsv --query \"[?name=='$ENDPT_NAME'][name]\" |  wc -l)\n",
    "                   if [[ ENDPOINT_EXISTS -ne 1 ]]; then\n",
    "                      echo \"endpoint does not exists\"\n",
    "                      az ml online-endpoint create --file '$(Pipeline.Workspace)/drop/chapter 9/src/deploy/create-endpoint.yml' -g $(resourceGroup) -w $(wsName)\n",
    "                      echo \"creating online deployment\"\n",
    "                      az ml online-deployment create --name $NEW_DEPLOYMENT_NAME -f '$(Pipeline.Workspace)/drop/chapter 9/src/deploy/model_deployment.yml' -g $(resourceGroup) -w $(wsName) \n",
    "                      echo \"updating online endpoint tags\"\n",
    "                      az ml online-endpoint update -n $ENDPT_NAME --set tags.prod=$NEW_DEPLOYMENT_NAME  --traffic \"$NEW_DEPLOYMENT_NAME=100\" -g $(resourceGroup) -w $(wsName)\n",
    "                      exit 0\n",
    "                  else\n",
    "                      echo \"endpoint exists, get deployment that already exists \"\n",
    "                      PROD_DEPLOYMENT=$(az ml online-endpoint show -n $ENDPT_NAME -g $(resourceGroup) -w $(wsName) -o tsv --query \"tags.prod\")\n",
    "                      echo $PROD_DEPLOYMENT\n",
    "                      az ml online-deployment create -g $(resourceGroup) -w $(wsName) --name $NEW_DEPLOYMENT_NAME -f \"$(Pipeline.Workspace)/drop/chapter 9/src/deploy/model_deployment.yml\" \n",
    "                      #test online-deployment created with sample file.\n",
    "                      echo \"test new endpoint\"\n",
    "                      az ml online-endpoint invoke -n $ENDPT_NAME --deployment $NEW_DEPLOYMENT_NAME --request-file \"$(Pipeline.Workspace)/drop/chapter 9/data/request.json\"\n",
    " \n",
    "                      echo \"---------------------------------------------------------------------------\"\n",
    "                      echo \"update tag and traffic\"\n",
    "                      az ml online-endpoint update -g $(resourceGroup) -w $(wsName) -n $ENDPT_NAME --set tags.prod=$NEW_DEPLOYMENT_NAME\n",
    "            \n",
    "                      echo \"---------------------------------------------------------------------------\"\n",
    "                      echo \"set traffic for origin endpoint to 0 - should be updated, but for completeness\"\n",
    "                      az ml online-endpoint update -g $(resourceGroup) -w $(wsName) -n $ENDPT_NAME --traffic \"$NEW_DEPLOYMENT_NAME=100 $PROD_DEPLOYMENT=0\"  --set tags.prod=$NEW_DEPLOYMENT_NAME\n",
    "\n",
    "                      echo \"---------------------------------------------------------------------------\"\n",
    "                      echo \"check online endpoints deployed\"\n",
    "                      az ml online-endpoint list --query \"[].{Name:name}\"  --output table --resource-group $(resourceGroup) --workspace-name $(wsName)\n",
    "                      echo \"delete old online endpoints deployed\"\n",
    "                      az ml online-deployment delete -g $(resourceGroup) -w $(wsName) --endpoint $ENDPT_NAME --name $PROD_DEPLOYMENT --yes --no-wait\n",
    "                \n",
    "                  fi   \n",
    "            name: 'deploymodel'\n",
    "            displayName: 'deploymodel'          \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
